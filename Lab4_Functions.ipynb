{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-tivity 4: CS5062 Overfitting and How to Prevent It \n",
    "\n",
    "## Name: Martin Power\n",
    "## ID      : 9939245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "# folds - number of folds, i.e. the value of k, for k-fold cross-validation\n",
    "# p - predictor attributes\n",
    "# t - target attribute for 2-class classification\n",
    "# classifier - a binary probabilistic classifier;\n",
    "#              it is assumed that the there are two classes: 0 and 1\n",
    "#              and the classifier learns to predict probabilities for the examples to belong to class 0 \n",
    "#              as well as probabilities for the examples to belong to class 1\n",
    "# mean_fpr - an array of equally spaced fpr values to be used for interpolating the tpr values\n",
    "\n",
    "# Output\n",
    "\n",
    "# _accuracies - average accuracy for each cross-validation run\n",
    "# _f1_scores - F1 score for each cross-validation run \n",
    "# _tprs - a matrix of true positive rates, each row corresponds to a cross-validation run \n",
    "#         and contains 100 values, corresponding to equally spaced false positive rates in the array mean_fpr\n",
    "# _aucs - areas under the curve, one per cross-validation run\n",
    "\n",
    "def evaluate_classifier(folds, p, t, classifier, mean_fpr):\n",
    "    _accuracies = np.array([])\n",
    "    _f1_scores = np.array([])\n",
    "    _tprs = np.empty(shape=[0,mean_fpr.shape[0]])\n",
    "    _aucs = np.array([])\n",
    "    \n",
    "    # cv is a k-fold cross-valiatidation object\n",
    "    cv = StratifiedKFold(n_splits=folds)\n",
    "    \n",
    "    for train, test in cv.split(p, t):\n",
    "        # train the classifier and compute the classes for the test set\n",
    "        _model = classifier.fit(p[train], t[train])\n",
    "        _probabilities = _model.predict_proba(p[test])\n",
    "        _predictions = _model.predict(p[test])\n",
    "        \n",
    "        # compute accuracy\n",
    "        _accuracies = np.append(_accuracies, accuracy_score(t[test], _predictions))\n",
    "        \n",
    "        # compute f1 score\n",
    "        _f1_scores = np.append(_f1_scores, f1_score(t[test], _predictions))\n",
    "    \n",
    "        # compute fpr and tpr values for various thresholds \n",
    "        # by comparing the true target values to the predicted probabilities for class 1\n",
    "        _fpr, _tpr, _thresholds = roc_curve(y_true = t[test], y_score = _probabilities[:, 1])\n",
    "                        \n",
    "        # compute true positive rates for the values in the array mean_fpr\n",
    "        _tpr_transformed = np.array([interp(mean_fpr, _fpr, _tpr)])\n",
    "        _tprs = np.concatenate((_tprs, _tpr_transformed), axis=0)\n",
    "    \n",
    "        # compute the area under the curve\n",
    "        _aucs = np.append(_aucs, auc(_fpr, _tpr))\n",
    "        \n",
    "    return _accuracies, _f1_scores, _tprs, _aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "# folds - number of folds, i.e. the value of k, for k-fold cross-validation\n",
    "# p - predictor attributes\n",
    "# t - target attribute for 2-class classification\n",
    "# classifier - a binary probabilistic classifier;\n",
    "#              it is assumed that the there are two classes: 0 and 1\n",
    "#              and the classifier learns to predict probabilities for the examples to belong to class 0 \n",
    "#              as well as probabilities for the examples to belong to class 1\n",
    "# mean_fpr - an array of equally spaced fpr values to be used for interpolating the tpr values\n",
    "# mean_tpr - an array of equally spaced tpr values to be used for interpolating the precision values\n",
    "\n",
    "# Output\n",
    "\n",
    "# _accuracies - average accuracy for each cross-validation run\n",
    "# _f1_scores - F1 score for each cross-validation run \n",
    "# _tprs - a matrix of true positive rates, each row corresponds to a cross-validation run \n",
    "#         and contains 100 values, corresponding to equally spaced false positive rates in the array mean_fpr\n",
    "# _aucs - areas under the curve, one per cross-validation run\n",
    "# _prcs - a matrix of preision values, each row corresponds to a cross-validation run \n",
    "#         and contains 100 values, corresponding to equally spaced true positive rates (recall) in the array mean_tpr\n",
    "\n",
    "def evaluate_classifier_new(folds, p, t, classifier, mean_fpr, mean_tpr):\n",
    "    _accuracies = np.array([])\n",
    "    _f1_scores = np.array([])\n",
    "    _tprs = np.empty(shape=[0,mean_fpr.shape[0]])\n",
    "    _aucs = np.array([])\n",
    "    \n",
    "    _precisions = np.array([])\n",
    "    _recalls = np.array([])\n",
    "    \n",
    "    _prcs = np.empty(shape=[0,mean_tpr.shape[0]])\n",
    "    _pr_aucs = np.array([])\n",
    "    \n",
    "\n",
    "    # cv is a k-fold cross-valiatidation object\n",
    "    cv = StratifiedKFold(n_splits=folds)\n",
    "    \n",
    "    for train, test in cv.split(p, t):\n",
    "        # train the classifier and compute the classes for the test set\n",
    "        _model = classifier.fit(p[train], t[train])\n",
    "        _probabilities = _model.predict_proba(p[test])\n",
    "        _predictions = _model.predict(p[test])\n",
    "        \n",
    "        # compute accuracy\n",
    "        _accuracies = np.append(_accuracies, accuracy_score(t[test], _predictions))\n",
    "        \n",
    "        # compute f1 score\n",
    "        _f1_scores = np.append(_f1_scores, f1_score(t[test], _predictions))\n",
    "        \n",
    "        # compute precision\n",
    "        _precisions = np.append(_precisions, precision_score(t[test], _predictions))\n",
    "        \n",
    "        # compute recalls\n",
    "        _recalls = np.append(_recalls, recall_score(t[test], _predictions))\n",
    "    \n",
    "        # compute fpr and tpr values for various thresholds \n",
    "        # by comparing the true target values to the predicted probabilities for class 1\n",
    "        _fpr, _tpr, _thresholds = roc_curve(y_true = t[test], y_score = _probabilities[:, 1])\n",
    "                        \n",
    "        # compute true positive rates for the values in the array mean_fpr\n",
    "        _tpr_transformed = np.array([interp(mean_fpr, _fpr, _tpr)])\n",
    "        _tprs = np.concatenate((_tprs, _tpr_transformed), axis=0)\n",
    "    \n",
    "        # compute the area under the curve\n",
    "        _aucs = np.append(_aucs, auc(_fpr, _tpr))\n",
    "        \n",
    "        # compute precision (prc) and recall (rec) values for various thresholds \n",
    "        # by comparing the true target values to the predicted probabilities for class 1\n",
    "        _prc, _rec, _pr_thresholds = precision_recall_curve(y_true = t[test], probas_pred = _probabilities[:, 1])\n",
    "            \n",
    "        # compute precision values for the values in the array mean_tpr\n",
    "        # _prc_transformed = np.array([interp(mean_tpr, _rec, _prc)])\n",
    "        _prc_transformed = np.array([interp(mean_tpr, _rec[::-1], _prc[::-1])])\n",
    "        _prcs = np.concatenate((_prcs, _prc_transformed), axis=0)\n",
    "\n",
    "        # compute the area under the precision-recall curve\n",
    "        _pr_aucs = np.append(_pr_aucs, auc(_rec, _prc))\n",
    "\n",
    "        \n",
    "#         print(\"Actual\")\n",
    "#         print(_rec[::-1])\n",
    "#         print(_prc[::-1])\n",
    "        \n",
    "#         plt.plot(_rec, _prc)\n",
    "#         plt.show()\n",
    "        \n",
    "#         print(\"Interpolated\")\n",
    "#         print(mean_tpr[0:,])\n",
    "#         print(_prc_transformed)\n",
    "#         #plt.plot(mean_tpr, _prc_transformed)\n",
    "#         #plt.show()\n",
    "#         plt.plot(mean_tpr.reshape(100,), _prc_transformed.reshape(100,))\n",
    "#         plt.show()\n",
    "#         print(mean_tpr.shape)\n",
    "#         print(_prc_transformed.shape)\n",
    "        \n",
    "    return _accuracies, _f1_scores, _precisions, _recalls, _tprs, _aucs, _prcs, _pr_aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot an ROC curve for each cross-validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cv_folds(mean_fpr, tprs, aucs, classifier_name):\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    \n",
    "    for i in range(0, aucs.shape[0]):\n",
    "        plt.plot(mean_fpr, tprs[i,:], lw=1, alpha=0.3,label='fold %d (AUC = %0.2f)' % (i, aucs[i]))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curves for %d cross-validation folds: %s' % (aucs.shape[0], classifier_name))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a mean curve for all cross-validation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_mean(mean_fpr, tprs, aucs, classifier_name):\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    # ROC curve - mean curve for all cross-validation runs\n",
    "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    # colour in grey the area of the standard deviation from the mean tpr\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Mean ROC curve for all cross-validation runs: ' + classifier_name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ROC curves for multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_multiple_classifiers(mean_fpr, tprs, aucs, classifier_names):\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    \n",
    "    for i in range(0, aucs.shape[0]):\n",
    "        plt.plot(mean_fpr, tprs[i,:], lw=2, alpha=0.8,label='%s (AUC = %0.2f)' % (classifier_names[i], aucs[i]))\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curves for multiple classifiers')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a  Preision Recall curve for each cross-validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_cv_folds(mean_tpr, prcs, aucs, target, classifier_name):\n",
    "    \n",
    "    # Determine No Skill Value\n",
    "    total_pos = ((target == 1).sum())\n",
    "    total_neg = ((target != 1).sum())\n",
    "    \n",
    "    no_skill = total_pos/(total_pos+total_neg)\n",
    "    print(\"No Skill = \", no_skill)\n",
    "    \n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', lw=2, color='r', label='No Skill', alpha=.8)\n",
    "    \n",
    "    for i in range(0, aucs.shape[0]):\n",
    "        plt.plot(mean_tpr, prcs[i,:], lw=1, alpha=0.3,label='fold %d (AUC = %0.2f)' % (i, aucs[i]))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision Recall for %d cross-validation folds: %s' % (aucs.shape[0], classifier_name))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a mean Precision Recall curve for all cross-validation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_mean(mean_tpr, prcs, aucs, target, classifier_name):\n",
    "    # Determine No Skill Value\n",
    "    total_pos = ((target == 1).sum())\n",
    "    total_neg = ((target != 1).sum())\n",
    "    \n",
    "    no_skill = total_pos/(total_pos+total_neg)\n",
    "    print(\"No Skill = \", no_skill)\n",
    "    \n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', lw=2, color='r', label='No Skill', alpha=.8)\n",
    "\n",
    "    mean_prc = np.mean(prcs, axis=0)\n",
    "    mean_auc = auc(mean_tpr, mean_prc)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    # ROC curve - mean curve for all cross-validation runs\n",
    "    plt.plot(mean_tpr, mean_prc, color='b',\n",
    "             label=r'Mean PR (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    # colour in grey the area of the standard deviation from the mean tpr\n",
    "    std_prc = np.std(prcs, axis=0)\n",
    "    prcs_upper = np.minimum(mean_prc + std_prc, 1)\n",
    "    prcs_lower = np.maximum(mean_prc - std_prc, 0)\n",
    "    plt.fill_between(mean_tpr, prcs_lower, prcs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Mean PR curve for all cross-validation runs: ' + classifier_name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Precision Recall curves for multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_multiple_classifiers(mean_tpr, prcs, aucs, target, classifier_names):\n",
    "    \n",
    "    # Determine No Skill Value\n",
    "    total_pos = ((target == 1).sum())\n",
    "    total_neg = ((target != 1).sum())\n",
    "    \n",
    "    no_skill = total_pos/(total_pos+total_neg)\n",
    "    print(\"No Skill = \", no_skill)\n",
    "    \n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', lw=2, color='r', label='No Skill', alpha=.8)\n",
    "    \n",
    "    for i in range(0, aucs.shape[0]):\n",
    "        plt.plot(mean_tpr, prcs[i,:], lw=2, alpha=0.8,label='%s (AUC = %0.2f)' % (classifier_names[i], aucs[i]))\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('PR curves for multiple classifiers')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
