{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Details\n",
    "\n",
    "Name:Michel Danjou\n",
    "\n",
    "ID:18263461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All necessary imports are provided. Please do not add further imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the exact same data you used for the regression in E-tivity 3, but this time you know exactly what function generated the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(X):\n",
    "    #return: 0.5X^5-0.5X^3-4.25X^2+5.125X-0.4375\n",
    "    return 0.5*((X-.5)-10*(X-.5)**2-(X-.5)**3+X**5)+1\n",
    "\n",
    "points = 100\n",
    "X = np.linspace(0, 1, points) # 100 values between 0 and 100\n",
    "noise=np.random.random(points)/4\n",
    "y_nf = target(X) #noise free target\n",
    "y = y_nf+noise #noisy target\n",
    "\n",
    "plt.plot(X,y,'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of linear regression with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_reg(X,y,l):\n",
    "    n = len(X)\n",
    "    m=X.T.dot(X)\n",
    "    return np.linalg.inv(m+l*np.identity(m.shape[0])).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(w,X):\n",
    "    # Calculation of outputs given weights and data (X). Note that X needs to contain the bias of 1. \n",
    "    out=[]\n",
    "    for x in X:\n",
    "        out.append(w.T.dot(x))\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error(w,X,y):\n",
    "    # Calculate the error as the mean squared error\n",
    "    pred = lin_reg(w,X)\n",
    "    return math.sqrt((pred-np.array(y)).dot(pred-np.array(y))/len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create higher order features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transPoly(X, power):\n",
    "    # Extend the data in X with a bias (1) and powers of the feature up to 'power'\n",
    "    ones = np.ones((X.shape[0],1))\n",
    "    extra=[]\n",
    "    for x in X:\n",
    "        row=[]\n",
    "        for p in range(2,power+1):\n",
    "            row.append(x**p)\n",
    "        extra.append(row)\n",
    "    return np.concatenate((ones, X.reshape(len(X),1),np.array(extra)),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over lambda, train the model and calculate Eout. Plot the latter versus lambda to see how much regularization is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ein=[]\n",
    "eout=[]\n",
    "weights=[]\n",
    "\n",
    "lambdas = np.linspace(0,1,1000)\n",
    "X_trans = transPoly(X,50)\n",
    "# random_state fixed to get reproducible (and 'good'! :-)) results\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trans, y, train_size=0.9, random_state =5)\n",
    "for l in lambdas:    \n",
    "    w = weights_reg(X_train,y_train,l)\n",
    "    ein.append(calc_error(w,X_train,y_train))\n",
    "    eout.append(calc_error(w,X_test,y_test))\n",
    "    weights.append(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.1 \n",
    "Using the above code, find the optimum regularization parameter, l. In order to do so, you will need to decide on a suitable list of l values. Pick at most 1000 values! Once you have found the optimal l, compare the Eout obtained with this l with the Eout obtained without regularization. You should see an impressive improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ein, eout for lambda > 0\n",
    "plt.plot(lambdas[1:],ein[1:], label='ein')\n",
    "plt.plot(lambdas[1:],eout[1:], label='eout')\n",
    "\n",
    "plt.xlabel(\"lambdas\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.title(\"Ein and Eout for lambda > 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is showing that the lowest Ein and Eout are obtained with the **lowest lambda**.\n",
    "Interestingly the Eout and Ein curves sometimes cross resulting in Eout being lower than Ein when lambda tends towards 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_eout_index = np.argmin(eout)\n",
    "best_lambda = lambdas[lowest_eout_index]\n",
    "\n",
    "print(\"lowest_eout_index          :\", lowest_eout_index)\n",
    "print(\"Best lambda used           :\", best_lambda)\n",
    "print(\"Eout without regularization:\", eout[0])\n",
    "print(\"Eout with regularization   :\", eout[lowest_eout_index])\n",
    "print(\"Difference                 :\", eout[0] - eout[lowest_eout_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4.2 Provide the following plots:\n",
    "\n",
    "  * A plot showing the estimate of Eout vs l on the domain [0,1]\n",
    "  * A plot with three sub-plots containing an overfitted model, a model with the optimal l, and an underfitted model. \n",
    "  \n",
    "In each plot, you should plot the target function in addition to the final hypothesis. Choose values for l that clearly demonstrate over- and underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lambdas, eout, label='eout')\n",
    "plt.plot(lambdas[0], eout[0], marker='x', color='red',  markersize=12, label=\"Error with no regularization.\")\n",
    "plt.xlim(0,1)\n",
    "plt.xlabel(\"lambdas\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.title(\"Eout by lambda (where lambda> 0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_polynom(weights, X):\n",
    "    h = weights[0]\n",
    "    for i in np.arange(1, len(weights)):\n",
    "        h += weights[i]*X ** i    \n",
    "    return h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0,1,1000)\n",
    "\n",
    "def experiment(order, l):\n",
    "#    X_trans = transPoly(X,order)\n",
    "\n",
    "    # random_state fixed to get reproducible (and 'good'! :-)) results\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_trans, y, train_size=0.9, random_state =5)\n",
    "\n",
    "    weights = weights_reg(X_train,y_train,l)\n",
    "    ein = calc_error(weights, X_train,y_train)\n",
    "    eout = calc_error(weights, X_test,y_test)\n",
    "    \n",
    "    return ein, eout, weights\n",
    "\n",
    "plt.figure(figsize=(8, 10)) \n",
    "    \n",
    "ls = [0, best_lambda, 10]\n",
    "for i in range(1,4):\n",
    "    subplt = int(str(31)+str(i))\n",
    "    \n",
    "    plt.subplot(subplt)\n",
    "    ein, eout, weights = experiment(50, ls[i-1])\n",
    "    pred = calc_polynom(weights, X)\n",
    "        \n",
    "    plt.plot(X,y,'r.')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    lbl = 'lambda=' + str(ls[i-1])\n",
    "    plt.title('y function estimated using ' + lbl)\n",
    "    plt.plot(X, pred  , 'g', label=lbl)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "  * Use the code from Task 4 and perform the same analysis using 10-fold cross-validation. \n",
    "  * Use Scikitlearn’s KFOLD for this and think carefully about the parameters you use in KFOLD. \n",
    "  * Note that the data set provided is ordered by increasing X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0,1,1000)\n",
    "\n",
    "def experiment(order, l):\n",
    "    ein=[]\n",
    "    eout=[]\n",
    "    weights=[]\n",
    "    \n",
    "    # TODO: Medidate on Pep's hint: random_state fixed to get reproducible (and 'good'! :-)) results\n",
    "    kf = KFold(n_splits=10)\n",
    "    kf.get_n_splits(X)\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X_trans[train_index], X_trans[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        w = weights_reg(X_train,y_train,l)\n",
    "        weights.append(w)\n",
    "        ein.append(calc_error(w,X_train,y_train))\n",
    "        eout.append(calc_error(w,X_test,y_test))\n",
    "    \n",
    "    # Validation. Find the lowest Eout and best lambda\n",
    "    lowest_eout_index = np.argmin(eout)\n",
    "    best_lambda = lambdas[lowest_eout_index]\n",
    "\n",
    "    return ein[lowest_eout_index], eout[lowest_eout_index], weights[lowest_eout_index]\n",
    "\n",
    "plt.figure(figsize=(8, 10)) \n",
    "    \n",
    "ls = [0, best_lambda, 10]\n",
    "for i in range(1,4):\n",
    "    subplt = int(str(31)+str(i))\n",
    "    \n",
    "    plt.subplot(subplt)\n",
    "    ein, eout, weights = experiment(50, ls[i-1])\n",
    "    pred = calc_polynom(weights, X)\n",
    "        \n",
    "    plt.plot(X,y,'r.')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    lbl = 'lambda=' + str(ls[i-1])\n",
    "    plt.title('y function estimated using ' + lbl)\n",
    "    plt.plot(X, pred  , 'g', label=lbl)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Consider shuffling the X_trans.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 (CE6002)\n",
    "Use scikit-learn’s RidgeCV and LassoCV to obtain optimum l using the same transformed data (i.e. features up to 50th order).\n",
    "\n",
    "Observe the resulting Eout, and l compare these to previous results. What do you see? Is this what you expected?\n",
    "Compare the weights obtained with RidgeCV to those obtained with LassoCV. What do you see? Is this what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "  * https://scikit-learn.org/stable/auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py\n",
    "  * https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html\n",
    "  * https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b \n",
    "    * Describes how RidgeVC and LassoCV perform better at reducting over-fitting compared to simple Linear regression. \n",
    "\n",
    "What is the difference between Ridge and RidgeCV?\n",
    "  * https://stats.stackexchange.com/questions/262640/the-results-of-cv-on-ridge-are-different-than-the-results-of-ridgecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_regression(title, clf):\n",
    "    score = clf.score(X_trans, y) \n",
    "    pred = clf.predict(X_trans)\n",
    "    params = clf.get_params\n",
    "\n",
    "    plt.plot(X,y,'r.')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    lbl = title\n",
    "    plt.title('y function estimated using ' + lbl)\n",
    "    plt.plot(X, pred  , 'g', label=lbl)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"score :\", score)\n",
    "    print(\"params:\", params)\n",
    "    print(\"pred  :\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X_trans, y)\n",
    "alternate_regression(\"RidgeCV\", clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LassoCV(cv=10, random_state=0).fit(X_trans, y)\n",
    "alternate_regression(\"LassoCV\", clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Play with the parameters to remove the warning !!!!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
