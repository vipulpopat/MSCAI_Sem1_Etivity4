{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Details:\n",
    "\n",
    "Name: **Chelliah Kanthanathan**\n",
    "\n",
    "ID: **18263003**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "# folds - number of folds, i.e. the value of k, for k-fold cross-validation\n",
    "# p - predictor attributes\n",
    "# t - target attribute for 2-class classification\n",
    "# classifier - a binary probabilistic classifier;\n",
    "#              it is assumed that the there are two classes: 0 and 1\n",
    "#              and the classifier learns to predict probabilities for the examples to belong to class 0 \n",
    "#              as well as probabilities for the examples to belong to class 1\n",
    "# mean_fpr - an array of equally spaced fpr values to be used for interpolating the tpr values\n",
    "\n",
    "# Output\n",
    "\n",
    "# _accuracies - average accuracy for each cross-validation run\n",
    "# _f1_scores - F1 score for each cross-validation run \n",
    "# _tprs - a matrix of true positive rates, each row corresponds to a cross-validation run \n",
    "#         and contains 100 values, corresponding to equally spaced false positive rates in the array mean_fpr\n",
    "# _aucs - areas under the curve, one per cross-validation run\n",
    "\n",
    "def evaluate_classifier(folds, p, t, classifier, mean_fpr):\n",
    "    _accuracies = np.array([])\n",
    "    _f1_scores = np.array([])\n",
    "    _tprs = np.empty(shape=[0,mean_fpr.shape[0]])\n",
    "    _aucs = np.array([])\n",
    "    _precision = np.array([])\n",
    "    _recall = np.array([])\n",
    "    \n",
    "    # cv is a k-fold cross-valiatidation object\n",
    "    cv = StratifiedKFold(n_splits=folds)\n",
    "    \n",
    "    for train, test in cv.split(p, t):\n",
    "        # train the classifier and compute the classes for the test set\n",
    "        _model = classifier.fit(p[train], t[train])\n",
    "        _probabilities = _model.predict_proba(p[test])\n",
    "        _predictions = _model.predict(p[test])\n",
    "        \n",
    "        # compute accuracy\n",
    "        _accuracies = np.append(_accuracies, accuracy_score(t[test], _predictions))\n",
    "        \n",
    "        # compute f1 score\n",
    "        _f1_scores = np.append(_f1_scores, f1_score(t[test], _predictions))\n",
    "    \n",
    "        # compute fpr and tpr values for various thresholds \n",
    "        # by comparing the true target values to the predicted probabilities for class 1\n",
    "        _fpr, _tpr, _thresholds = roc_curve(y_true = t[test], y_score = _probabilities[:, 1])\n",
    "                        \n",
    "        # compute true positive rates for the values in the array mean_fpr\n",
    "        _tpr_transformed = np.array([interp(mean_fpr, _fpr, _tpr)])\n",
    "        _tprs = np.concatenate((_tprs, _tpr_transformed), axis=0)\n",
    "    \n",
    "        # compute the area under the curve\n",
    "        _aucs = np.append(_aucs, auc(_fpr, _tpr))\n",
    "        \n",
    "        # compute precision and recall\n",
    "        #_precision, _recall, _threshold = precision_recall_curve(t[test],_probabilities[:, 1])\n",
    "        _precision = np.append(_precision,precision_score(t[test],_predictions))\n",
    "        _recall = np.append(_recall,recall_score(t[test],_predictions))\n",
    "        \n",
    "    return _accuracies, _f1_scores, _tprs, _aucs, _precision, _recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot an ROC curve for each cross-validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cv_folds(mean_fpr, tprs, aucs, classifier_name):\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    \n",
    "    for i in range(0, aucs.shape[0]):\n",
    "        plt.plot(mean_fpr, tprs[i,:], lw=1, alpha=0.3,label='fold %d (AUC = %0.2f)' % (i, aucs[i]))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curves for %d cross-validation folds: %s' % (aucs.shape[0], classifier_name))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a mean curve for all cross-validation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_mean(mean_fpr, tprs, aucs, classifier_name):\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    # ROC curve - mean curve for all cross-validation runs\n",
    "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    # colour in grey the area of the standard deviation from the mean tpr\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Mean ROC curve for all cross-validation runs: ' + classifier_name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ROC curves for multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_multiple_classifiers(mean_fpr, tprs, aucs, classifier_names):\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    \n",
    "    for i in range(0, aucs.shape[0]):\n",
    "        plt.plot(mean_fpr, tprs[i,:], lw=2, alpha=0.8,label='%s (AUC = %0.2f)' % (classifier_names[i], aucs[i]))\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curves for multiple classifiers')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
